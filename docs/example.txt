ðŸ“Š Fluxo Completo:

1. UsuÃ¡rio pergunta: "o que Ã© RAG?"
                â†“
2. rag_agent.py chama:
   search_results = self.execute_tool("vector_search", query=query, k=3)
                â†“
3. vector_search.py busca no Qdrant e retorna:
   "1. (Score: 0.8) RAG significa Retrieval-Augmented Generation...
    2. (Score: 0.7) O RAG funciona em trÃªs etapas...
    3. (Score: 0.6) Componentes do Sistema RAG..."
                â†“

## Passos da V2
4. rag_agent.py passa para o LLM:
   llm_response = llm_manager.generate_response(query, search_results)
                â†“
5. llm.py monta o prompt:
   "Context: [documentos do Qdrant]
    Question: o que Ã© RAG?
    Answer:"
                â†“
6. LLM gera resposta BASEADA no contexto
                â†“
7. Retorna resposta natural ao usuÃ¡rio