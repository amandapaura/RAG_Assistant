# ğŸ¤– RAG Assistant - Sistema Conversacional Open Source

Sistema avanÃ§ado de assistente conversacional baseado em RAG (Retrieval-Augmented Generation) usando apenas tecnologias open source.

## âœ¨ CaracterÃ­sticas

- **ğŸ§  LLMs Open Source**: Utiliza modelos do Hugging Face
- **ğŸ” Busca Vetorial**: Qdrant para similarity search
- **ğŸŒ Multi-Agent**: Agentes especializados (RAG, Web Search, Weather, SQL)
- **ğŸ“Š AvaliaÃ§Ã£o Automatizada**: MÃ©tricas de context relevance, groundedness, PII detection
- **ğŸ•¸ï¸ LangGraph**: OrquestraÃ§Ã£o avanÃ§ada de workflows
- **ğŸ’¬ Interface Moderna**: Streamlit com chat em tempo real
- **ğŸ³ Docker First**: Deploy completo com docker-compose

## ğŸ“Š Fluxo Completo:

1. UsuÃ¡rio pergunta: "o que Ã© RAG?"
                â†“
2. rag_agent.py chama:
   search_results = self.execute_tool("vector_search", query=query, k=3)
                â†“
3. vector_search.py busca no Qdrant e retorna:
   "1. (Score: 0.8) RAG significa Retrieval-Augmented Generation...
    2. (Score: 0.7) O RAG funciona em trÃªs etapas...
    3. (Score: 0.6) Componentes do Sistema RAG..."
                â†“

## Passos da V2
4. rag_agent.py passa para o LLM:
   llm_response = llm_manager.generate_response(query, search_results)
                â†“
5. llm.py monta o prompt:
   "Context: [documentos do Qdrant]
    Question: o que Ã© RAG?
    Answer:"
                â†“
6. LLM gera resposta BASEADA no contexto
                â†“
7. Retorna resposta natural ao usuÃ¡rio

## ğŸš€ Quick Start
```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/amandapaura/RAG_Assistant.git
cd RAG_Assistant

# 2. Configure variÃ¡veis de ambiente
cp .env.example .env
# Edite .env com suas configuraÃ§Ãµes

# 3. Inicie com Docker
docker-compose up --build

# 4. Acesse a aplicaÃ§Ã£o
streamlit run app/main.py
```

